---
title: "131-HW4"
output:
  pdf_document: default
  html_document: default
---

```{r}
library(tidymodels)
library(ISLR) 
library(ISLR2) 
library(discrim)
library(poissonreg)
library(corrr)
library(klaR) 
library(dplyr)
library(MASS)
library(pROC)
library(tune)
tidymodels_prefer()
library(readr)
titanic <- read_csv("~/Downloads/homework-4/data/titanic.csv")
View(titanic)

```

# Question 1
```{r}
titanic$survived <- factor(titanic$survived, levels = c("Yes","No"))
titanic$pclass <- as.factor(titanic$pclass)

set.seed(891)

titanic_split <- initial_split(titanic, prop = 0.8, strata = survived)
titanic_train <- training(titanic_split)
titanic_test <- testing(titanic_split)

#verify 
dim(titanic_train)
dim(titanic_test)
```

titanic_train: 712 observations of 12 variables
titanic_test: 179 observations of 12 variables

```{r}
#linear model
lm_spec <- linear_reg() %>%
  set_mode("regression") %>%
  set_engine("lm")


#recipe 
titanic_recipe <- recipe(survived ~ pclass + sex + age + 
                           sib_sp + parch + fare, titanic_train) %>% 
  step_impute_linear(age, impute_with = imp_vars(sib_sp)) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact(~ starts_with("sex"):age + age:fare) %>%
  step_poly(., degree = tune())
```

# Question 2
``` {r}
#folds
titanic_folds <- vfold_cv(titanic_train, v=10)
titanic_folds

```

# Question 3

In Question 2, we are using the k-fold cross validation to estimate the skill of the model on new data. This resampling procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. Sometimes it is preferred to use this method over a simple test/train split because it is simple to understand and generally results in a less biased/optimistic estimate of the model skill. If we were to use the entire training set, the resampling method used would be the "validation set" approach. 

# Question 4
```{r}
#LOG
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

log_wkflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(titanic_recipe)

#LDA 
lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

lda_wkflow <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(titanic_recipe)

#QDA
qda_mod <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

qda_wkflow <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(titanic_recipe)

```
Typically, the k-fold cross-validation procedure involves fitting a model on all folds (training) but one (testing), so 3 models x 9 folds = 27 models total. 

# Question 5
```{r}
degree_grid <- grid_regular(degree(range = c(1, 10)), levels = 10)
degree_grid

#fit log
tune_res <- tune_grid(
  object = log_wkflow, 
  resamples = titanic_folds, 
  grid = degree_grid,
  control = control_grid(verbose = TRUE)
)
tune_res

#fit LDA
tune_res2 <- tune_grid(
  object = lda_wkflow, 
  resamples = titanic_folds, 
  grid = degree_grid,
  control = control_grid(verbose = TRUE)
)
tune_res2
#fit QDA

tune_res3 <- tune_grid(
  object = qda_wkflow, 
  resamples = titanic_folds, 
  grid = degree_grid,
  control = control_grid(verbose = TRUE)
)
tune_res3

```

# Question 6
```{r}
collect_metrics(tune_res)
collect_metrics(tune_res2)
collect_metrics(tune_res3)
```
The logistic regression model seemed to have performed the best seeing as it is the model with the best accuracy.

# Question 7
```{r}
select_by_one_std_err(tune_res, degree, metric = "accuracy")
best_degree <- select_by_one_std_err(tune_res, degree, metric = "accuracy")
final_wf <- finalize_workflow(log_wkflow, best_degree)
final_wf

final_fit <- fit(final_wf, titanic_train)
final_fit
```
# Question 8
```{r}
testing_pred <- 
  predict(final_fit, titanic_train) %>% 
  bind_cols(predict(final_fit, titanic_train, type = "prob")) %>% 
  bind_cols(titanic_train %>% 
              select(survived))
testing_pred

augment(final_fit, new_data = titanic_test) %>%
  accuracy(truth = survived, estimate = .pred_class)
```
The performance metrics from the test set are much closer to the performance metrics computed using resampling. Resampling allowed for the simulation of how well the model will perform on new data, and the test set acts as the final, unbiased check for the modelâ€™s performance. 

 
